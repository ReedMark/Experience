# VGG 与风格迁移算法原理

## 一、实验介绍

### 1.1 实验内容

上节课我们学习了卷积神经网络的基本原理，本节实验我们将学习用于图像风格迁移的经典的卷积神经网络模型 VGG，并用 caffe 提供的 `draw_net.py` 实现模型的可视化，本节实验我们也将学习图像风格转换的算法原理。

### 1.2 实验知识点

- 经典 CNN 模型 `VGG16`
- 图片风格算法原理

### 1.3 实验环境

- python 2.7
- caffe `实验楼环境内置`
- Xfce 终端

## 二、VGG

CNN 的经典结构始于 1998 年的 LeNet-5，成于 2012 年历史性的 AlexNet，从此大盛于图像相关领域，主要包括：

- LeNet-5，1998 年
- AlexNet，2012 年
- ZF-net，2013 年
- `GoogleNet`，2014 年
- `VGG`，2014 年
- ResNet，2015 年
- Deep Residual Learning，2015 年

vgg 和 googlenet 是 2014 年 `ImageNet` （图像分类大赛） 的双雄，这两类模型结构有一个共同特点是 Go deeper。

VGG 网络非常深，通常有 `16/19`层，称作 `VGG16` 和 `VGG19` ，卷积核大小为 `3 x 3` ，16 和 19 层的区别主要在于后面三个卷积部分卷积层的数量。

本实验主要讲解 VGG16。

### 2.1 可视化 VGG16

启动终端，进入 caffe 下的 python 目录：

```
$ cd /opt/caffe/python
$ ls

```

确认是否有 `draw_net.py` 文件。

获得我们提供的 VGG16 的网格配置文件 `vgg16_train_val.prototxt`。

```
$ sudo wget http://labfile.oss.aliyuncs.com/courses/861/vgg16_train_val.prototxt

```

安装依赖包，这些都是基于 python 的画图工具包。

```
$ sudo apt-get install python-pydot
$ sudo apt-get install graphviz

```

安装完毕，准备可视化 VGG16 模型。

```
$ python draw_net.py /opt/caffe/python/vgg16_train_val.prototxt ~/Desktop/vgg16.png

```

程序运行完成之后，你可以通过以下命令查看`vgg16.png`，这个即为可视化后的 VGG16。

```
$ cd ~/Desktop/
$ display vgg16.png

```

### 2.2 理解 VGG16

下图是官方给出的 VGG16 的数据表格：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1499242053443.png/wm)

首先我们先学会看懂一些式子的表达：

`Conv3-512` → 第三层卷积后维度变成 512；

`Conv3_2 s=2` → 第三层卷积层里面的第二子层，滑动步长等于 2（每次移动两个格子）

有了以上的知识可以开始剖析 `VGG16` 了。

由于篇幅的关系我们只推演从 `Input` 到 `Conv1`，再从 `Conv1` 过渡到 `Conv2` 的过程。

#### 2.2.2 从 `Input` 到 `Conv1`

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1499243533706.png/wm)

假如我们输入的是 `300x300`的 RGB 图像，那么输入层接受的输入就是 `300x300x3`个神经元。

图中的两个黄色表示卷积层，是 VGG16 网络结构十六层当中的第一层（Conv1_1）和第二层（Conv1_2），他们合称为 Conv1。由上节的知识，我们可以知道卷积核大小应为 `3x3x3`。经过卷积层的运算我们可以得到一维的 `298x298x1` 的矩阵（`因为卷积核也是三维所以结果是一维`）, 为了在下一层中可以继续用 `3x3` 的卷积核做卷积，我们需要进行 `Padding` 操作，将图像扩充为 `300x300x1`，而 `VGG16` 在 Conv1 这层中使用了 64 个卷积核，那么生成了 64 张对应不同卷积核的 `feature map`，这样就得到了上图最终的结果 `300x300x64`。

#### 2.2.2 从 `Conv1` 的 `Conv2` 过渡

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1499244375403.png/wm)

这一步操作使用 `Pooling` ，使用的 `池化卷积核`为 `2*2*64`，步长为 `2` ，那么滑动的矩阵本身没有重叠；刚好减半，第三维度 `64` 不变。

其实之后的步骤都是类似的，根据我们刚刚画出的 VGG16 或者官方给出的表格，理解每一层有什么卷积核？多少个？池化的大小？步长多少？是否需要 Padding？一步步思考，你就可以完全掌握 VGG16 的原理。

## 三、图像风格迁移 `Image Style Transfer`

以目前的深度学习技术，如果给定两张图像，完全有能力让计算机识别出图像具体内容。而图像的风格是一种很抽象的东西，人眼能够很有效地的辨别出不同画家不同流派绘画的风格，而在计算机的眼中，本质上就是一些像素，多层网络的实质其实就是找出更复杂、更内在的特性 (features)，所以图像的风格理论上可以通过多层网络来提取图像里面可能含有的一些有意思的特征。

### 3.1 简介

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498718520318.png?imageView/2/w/400/wm)

将一幅图像的风格转移到另外一幅图像上被认为是一个图像纹理转移问题，传统上一般采用的是一些非参方法，通过一些专有的固定的方法（提取图像的亮度、低频颜色信息、高频纹理信息）来渲染。但是这些方法的问题在于只能提取底层特征而非高层抽象特征，而且往往一个程序只能做某一种风格或者某一个场景。随着 CNN 的日渐成熟，图像风格迁移技术也迎来了一次变革。需要注意的是，最近的很多应用型的研究成果都是将 CNN 渗透进各个领域，从而在普遍意义上完成一次技术的升级。

### 3.2 方法

实现图像的风格转换我们需要下面几个原料：

- 一个训练好的神经网络 `VGG`
- 一张风格图像，用来计算它的风格 representation
- 一张内容图像，用来计算它的内容 representation，
- 一张噪声图像，用来迭代优化
- loss 函数，用来获得 loss

给定一张风格图像 **a** 和一张普通图像 **p**，风格图像经过 VGG 的时候在每个卷积层会得到很多 `feature maps`, 这些 feature maps 组成一个集合 *A*，同样的，普通图像 **p** 通过 VGG 的时候也会得到很多 feature maps，这些 feature maps 组成一个集合 *P*，然后生成一张随机噪声图像 **x**（在后面的实验中，其实就是普通图像 **p**）, 随机噪声图像 **x** 通过 VGG 的时候也会生成很多 feature maps，这些 feature maps 构成集合 *G* 和 *F* 分别对应集合 *A* 和 *P*, 最终的优化函数是希望调整 **x** , 让随机噪声图像 **x** 最后看起来既保持普通图像 **p** 的内容, 又有一定的风格图像 **a** 的风格。

可以进行风格转换的基础就是将内容和风格区分开来，接下来我们来看 CNN 如何做到这一点。

#### 3.2.1 内容表示 `Content Representation`

给定一个图像 **p**，卷积神经网络每层使用滤波器 (`卷积核 filter`) 对图像进行编码。在 CNN 中, 假设 `layer l` 有![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498723062016.png/wm)个 filters, 那么将会生成![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498723062016.png/wm)个 feature maps，每个 feature map 的维度为![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498723203690.png/wm) , ![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498723219261.png/wm)代表 feature map 的高与宽的乘积。所以每一层 feature maps 的集合可以表示为

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498722256122.png/wm)，![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498722910567.png/wm)表示第 `i 个 filter` 在 `position j` 上的激活值。

所以，我们可以给出 Content 的 `cost function`，其中，**p** 是内容图像，**x** 是生成图像 :

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498716488929.png/wm)

对其求导后对应的激活函数为 ：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498717823045.png/wm)

有了这个公式之后要怎么做呢？

使用现在公布的训练好的某些 CNN 网络，随机初始化一个输入图片大小的噪声图像 x，然后保持 CNN 参数不变，将内容图片 **p** 和随机图像 **x** 输入进网络，然后对 x 求导，这样，**x** 就会在内容上越来越趋近于** p**。

#### 3.2.2 风格表示 `Style Representation`

对于风格提取，我们需要用到 `Gram matrix`。

> Gram 矩阵是计算每个通道 i 的 feature map 与每个通道 j 的 feature map 的内积。这个值可以看作代表 i 通道的 feature map 与 j 通道的 feature map 的互相关程度。而它又为什么能代表图片风格呢？
>
> 
>
> ![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1499328372013.png/wm)
>
> 
>
> 假设我们要对梵高的星空图做风格提取，在神经网络中某一层中有一个滤波器专门检测尖尖的塔顶这样的东西，另一个滤波器专门检测黑色。又有一个滤波器负责检测圆圆的东西，又有一个滤波器用来检测金黄色。
>
> 对梵高的原图做 Gram 矩阵，谁的相关性会比较大呢？如上图所示，“尖尖的”和 “黑色” 总是一起出现的，它们的相关性比较高。而 “圆圆的” 和“金黄色”都是一起出现的，他们的相关性比较高。
>
> 因此在风格转移的时候，其实也在内容图（待风格转换的图）里去寻找这种 “匹配”，将尖尖的渲染为黑色（如塔尖），将圆圆的渲染为金黄色（如近圆的房顶）。如果我们承认 “图像的艺术风格就是其基本形状与色彩的组合方式” ，这样一个假设，那么 Gram 矩阵能够表征艺术风格就是理所当然的事情了。

风格的抽取仍然是以层为单位的，对于风格图像 **a**，为了建立风格的 representation，我们先利用 `Gram matrix` 去表示每一层各个 feature maps 之间的关系，![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498724780698.png/wm)，![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498724993068.png/wm)是 **x**的 feature maps i,j 的内积，![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498729063707.png/wm)是 **a** 的 feature maps i,j 的内积。

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498717830066.png/wm)

利用 Gram matrix，我们可以建立每一层的关于 style 的 `cost function`，

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498717839721.png/wm)

求偏导后对应的激励函数：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498717916990.png/wm)

结合所有层，可以得到总的 `cost` :

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498717841853.png/wm)

不考虑风格转换，只单独的考虑内容或者风格，可以看到如图所示：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498729931306.png/wm)

上半部分是风格重建，由图可见，越用高层的特征，风格重建的就越粗粒度化。下半部分是内容重建，由图可见，越是底层的特征，重建的效果就越精细，越不容易变形。

最后将 content 和 style 的 cost 相结合，最终可以得到

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498725318750.png/wm)

##### 3.2.3 风格转换

提取了内容和风格之后，我们就可以开始进行图片的风格迁移了。

具体过程如下图所示：

![此处输入图片的描述](https://dn-anything-about-doc.qbox.me/document-uid440821labid3126timestamp1498730087122.png/wm)

## 四、实验总结

本节实验，我们学习了

- 经典神经网络模型 VGG
- 图像风格迁移的算法原理基础

下节实验我们将亲自动手实现任意图片的风格转换。

## 五、课后习题

本节实验中，我们使用 draw_net.py 绘制出的结构图保存了参数信息，细节丰富，但是缺点是结构不是很清晰明了，这一点会在大型模型上的体现尤为明显，其实我们还可以用很多工具来实现网络模型的可视化操作，请使用在线工具 [QuickStart](http://ethereon.github.io/netscope/quickstart.html) 绘制 googlenet 的模型，并对照着可视化后的网络模型理解 googlenet 的操作原理。

## 六、参考文献

[1] [Johnson J, Alahi A, Fei-Fei L. Perceptual losses for real-time style transfer and super-resolution[C\]//European Conference on Computer Vision. Springer International Publishing, 2016: 694-711.](https://arxiv.org/abs/1603.08155)

[2] [Gatys L A, Ecker A S, Bethge M. A neural algorithm of artistic style[J\]. arXiv preprint arXiv:1508.06576, 2015.](https://arxiv.org/abs/1508.06576)